{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327d2950-a382-4253-9730-0eb21bd7bafa",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "Ans.  Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale the values of a feature so that they fall within a specified range, typically 0 to 1. This technique is useful in various machine learning algorithms where the scale of the features can significantly impact the performance of the model.\n",
    "\n",
    "The formula for min max scaling is:\n",
    "\n",
    "X_scaled = (X-X_min)/(X_max - X_min)\n",
    "\n",
    "where,\n",
    "X - the original value:\n",
    "X_min - minimum value in the dataset\n",
    "X_max - maximum value in the datset\n",
    "\n",
    "Example :\n",
    "\n",
    "Suppose we have a dataset with a single feature, and the values are:\n",
    "X=[10,20,30,40,50]\n",
    "\n",
    "To apply Min-Max scaling, we first find the minimum and maximum values of X:\n",
    "X_min = 10\n",
    "X_max = 50\n",
    "\n",
    "X[0]= (10-10)/(50-10) = 0\n",
    "\n",
    "X[1] = (20-10)/(50-10) = 0.25\n",
    "\n",
    "X[2] = (30-10)/(50-10) = 0.5\n",
    "\n",
    "X[3] = (40-10)/(50-10) = 0.75\n",
    "\n",
    "X[4] = (50-10)/(50-10) = 1\n",
    "\n",
    "Thus, the scaled values are:\n",
    "\n",
    "X_scaled=[0,0.25,0.5,0.75,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9bab8a-c298-4bbf-ab88-c1f87aef93ad",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "Ans. The Unit Vector technique, also known as normalization or vector normalization, is a feature scaling method where each feature vector is scaled so that its magnitude becomes 1. This technique ensures that all the data points lie on the surface of a hypersphere, which can be useful in various machine learning algorithms, particularly those involving distance metrics.\n",
    "\n",
    "Min-Max Scaling Rescales the data to a fixed range, typically [0, 1], based on the minimum and maximum values of the data.\n",
    "Unit Vector Scaling Scales the data based on the magnitude of the vector, making the vector length equal to 1.\n",
    "\n",
    "Formula of Unit vector:\n",
    "\n",
    "X_scalaed = X/||x||\n",
    "\n",
    "where , \n",
    "X is original feature vector\n",
    "\n",
    "||X|| is the magnitude of X\n",
    "\n",
    "Example,\n",
    "Consider a dataset with a single feature vector:\n",
    "\n",
    "X=[3,4]\n",
    "\n",
    "||X|| = underroot(3^2 + 4^2) = 5\n",
    "\n",
    "X_scaled = [3,4]/5 = [0.6,0.8]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c18a4c7-f051-43d2-9548-4382b0c23038",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "Ans. PCA is a dimensionality reduction technique that transforms a dataset into a set of linearly uncorrelated features called principal components. These components capture the most variance in the data, allowing for a reduced dimensionality while preserving important information.\n",
    "\n",
    "Steps in PCA\n",
    "\n",
    "Standardize the Data: Scale the data to have a mean of 0 and a variance of 1.\n",
    "\n",
    "Compute the Covariance Matrix: Understand the variance and relationships between features.\n",
    "\n",
    "Calculate Eigenvalues and Eigenvectors: Identify the directions (eigenvectors) and their importance (eigenvalues).\n",
    "\n",
    "Sort Eigenvalues and Eigenvectors: Order by descending eigenvalues.\n",
    "\n",
    "Form the Feature Vector: Select the top k eigenvectors.\n",
    "\n",
    "Transform the Data: Multiply the original data by the feature vector to get the reduced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9906d6-ae8a-400c-9bdf-15b15e6640ef",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "Ans. Relationship between PCA and Feature Extraction:\n",
    "\n",
    "Feature extraction involves transforming raw data into a set of features that better represent the underlying structure of the data, which can enhance the performance of machine learning models. PCA aids in feature extraction by identifying the directions (principal components) that capture the maximum variance in the data. These principal components can be used as new features, reducing dimensionality while retaining the most important information.\n",
    "\n",
    "How PCA is Used for Feature Extraction\n",
    "\n",
    "Standardize the Data: Ensure all features have the same scale.\n",
    "\n",
    "Compute the Covariance Matrix: Understand how features vary with respect to each other.\n",
    "\n",
    "Calculate Eigenvalues and Eigenvectors: Identify principal components.\n",
    "\n",
    "Sort and Select Top k Components: Choose the principal components that capture the most variance.\n",
    "\n",
    "Transform the Data: Project the original data onto the selected principal components to get the new features.\n",
    "\n",
    "Example: Consider a dataset with three data points and two features:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a03ece7b-3d4a-4609-b316-e742410c1c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[2.5 2.4]\n",
      " [0.5 0.7]\n",
      " [2.2 2.9]]\n",
      "\n",
      "Standardized Data:\n",
      " [[ 0.87056284  0.4247954 ]\n",
      " [-1.40047065 -1.38058503]\n",
      " [ 0.52990781  0.95578964]]\n",
      "\n",
      "Principal Component:\n",
      " [[-0.70710678 -0.70710678]]\n",
      "\n",
      "Explained Variance Ratio:\n",
      " [0.96829339]\n",
      "\n",
      "Transformed Data:\n",
      " [[-0.91595659]\n",
      " [ 1.96650334]\n",
      " [-1.05054674]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Given dataset\n",
    "X = np.array([[2.5, 2.4],\n",
    "              [0.5, 0.7],\n",
    "              [2.2, 2.9]])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=1)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "print(\"Original Data:\\n\", X)\n",
    "print(\"\\nStandardized Data:\\n\", X_standardized)\n",
    "print(\"\\nPrincipal Component:\\n\", pca.components_)\n",
    "print(\"\\nExplained Variance Ratio:\\n\", pca.explained_variance_ratio_)\n",
    "print(\"\\nTransformed Data:\\n\", X_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be398d9a-58fa-4c40-b3b4-5a987a38f88a",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "Ans.  To build a recommendation system for a food delivery service, preprocessing the data is a crucial step. Given features such as price, rating, and delivery time, Min-Max scaling can be used to normalize these features, ensuring they are on a similar scale and thus improving the performance of the recommendation algorithm.\n",
    "\n",
    "Steps to Use Min-Max Scaling\n",
    "\n",
    "Understand the Data:\n",
    "\n",
    "Inspect the dataset to understand the range of each feature (price, rating, delivery time).\n",
    "\n",
    "Calculate Minimum and Maximum Values:\n",
    "\n",
    "For each feature, calculate the minimum and maximum values.\n",
    "\n",
    "Apply Min-Max Scaling:\n",
    "\n",
    "Use the Min-Max scaling formula to transform each feature to a specified range, typically [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0b993-203e-4ef9-943a-c0a70964b0fc",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "Ans. To build a model to predict stock prices using a dataset with many features (such as company financial data and market trends), Principal Component Analysis (PCA) can be employed to reduce dimensionality. This process simplifies the dataset, reduces computational cost, and can improve model performance by removing noise and multicollinearity.\n",
    "\n",
    "Steps to Use PCA for Dimensionality Reduction\n",
    "\n",
    "Standardize the Data:\n",
    "\n",
    "Since PCA is affected by the scales of the features, standardize the dataset to have a mean of 0 and a variance of 1.\n",
    "\n",
    "Compute the Covariance Matrix:\n",
    "\n",
    "Calculate the covariance matrix to understand how the features vary from the mean with respect to each other.\n",
    "\n",
    "Calculate Eigenvalues and Eigenvectors:\n",
    "\n",
    "Compute the eigenvalues and eigenvectors of the covariance matrix to identify the principal components.\n",
    "\n",
    "Sort Eigenvalues and Eigenvectors:\n",
    "\n",
    "Sort the eigenvalues in descending order and sort the eigenvectors accordingly. The eigenvectors corresponding to the highest eigenvalues are the principal components.\n",
    "\n",
    "Choose the Number of Principal Components:\n",
    "\n",
    "Decide how many principal components to keep. This can be based on the explained variance (e.g., keeping enough components to explain 95% of the variance).\n",
    "\n",
    "Transform the Data:\n",
    "\n",
    "Multiply the original standardized dataset by the selected eigenvectors (principal components) to obtain the reduced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c96ae9-928e-4270-9dd2-96d097751f4a",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "Ans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55edc71c-58f9-4399-a4f6-6922a74f90cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [ 1  5 10 15 20]\n",
      "Scaled Data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Min and Max values of the original dataset\n",
    "X_min = data.min()\n",
    "X_max = data.max()\n",
    "\n",
    "# Desired range\n",
    "a, b = -1, 1\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaled_data = (data - X_min) / (X_max - X_min) * (b - a) + a\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Scaled Data:\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1322a14a-409e-4383-a7a6-a2a70eca0f1a",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "Ans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da9048e6-c207-4a6f-8b80-e0c7a188c2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance by each principal component:\n",
      " [6.51732034e-01 3.05663577e-01 4.26043893e-02 2.12883002e-33\n",
      " 8.42951699e-38]\n",
      "\n",
      "Cumulative Explained Variance:\n",
      " [0.65173203 0.95739561 1.         1.         1.        ]\n",
      "\n",
      "Number of principal components to retain to explain 95% of the variance: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.DataFrame({\n",
    "    'height': [170, 160, 180, 175, 165],\n",
    "    'weight': [70, 60, 80, 75, 65],\n",
    "    'age': [30, 25, 35, 28, 40],\n",
    "    'gender': [0, 1, 0, 1, 0],  # 0 for male, 1 for female\n",
    "    'blood pressure': [120, 110, 130, 125, 115]\n",
    "})\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "data_pca = pca.fit_transform(data_standardized)\n",
    "\n",
    "# Explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Display results\n",
    "print(\"Explained Variance by each principal component:\\n\", explained_variance)\n",
    "print(\"\\nCumulative Explained Variance:\\n\", cumulative_explained_variance)\n",
    "\n",
    "# Determine number of components to retain\n",
    "num_components = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "print(f\"\\nNumber of principal components to retain to explain 95% of the variance: {num_components}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c81d5c-4146-44e0-82eb-bbdbab65d6d9",
   "metadata": {},
   "source": [
    "From the example output, you would retain 4 principal components to explain 95% of the variance. The exact number may vary based on your specific dataset. The goal is to retain enough components to capture most of the variance while reducing the dimensionality of the data. This helps in simplifying the model, reducing computational cost, and potentially improving the model's performance by eliminating noise and irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b76074e-72ed-470f-8a9d-fab86d162b81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
