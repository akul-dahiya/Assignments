{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93df7500-ea90-4ec0-a0a2-1ac2dede2aa7",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Ans. Lasso Regression is a linear regression technique that adds L1 regularization to the standard loss function. This penalty term shrinks some coefficients to zero, effectively performing feature selection and creating a sparse model.\n",
    "\n",
    "Differences from Other Techniques:\n",
    "\n",
    "Ridge Regression: Uses L2 regularization, which shrinks coefficients but does not set them to zero, so all features are included.\n",
    "\n",
    "Elastic Net: Combines L1 and L2 regularization, balancing feature selection and coefficient shrinkage.\n",
    "\n",
    "Standard Linear Regression: No regularization, so it may overfit and include all features without selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a597f-7083-437a-8ec5-ffba6f86a932",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "Ans.  The main advantage of using Lasso Regression in feature selection is its ability to perform automatic feature selection by shrinking some coefficients to exactly zero. This results in a sparse model where only a subset of the features are included, which can lead to:\n",
    "\n",
    "Simpler Models: Reduced complexity with fewer features, making the model easier to interpret.\n",
    "\n",
    "Improved Generalization: Less overfitting by excluding irrelevant or redundant features.\n",
    "\n",
    "Enhanced Performance: Potentially better predictive performance by focusing on the most relevant features.\n",
    "\n",
    "This automatic feature selection makes Lasso Regression particularly useful when dealing with high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28b4ff-95ec-4e98-b93b-c7dda8d702f3",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "Ans. In Lasso Regression, the coefficients are interpreted as follows:\n",
    "\n",
    "Magnitude and Direction: Each coefficient represents the influence of the corresponding feature on the target variable. A positive coefficient indicates a positive relationship, while a negative coefficient indicates a negative relationship.\n",
    "\n",
    "Zero Coefficients: Due to Lasso’s feature selection property (L1 regularization), some coefficients may be exactly zero. This means that the corresponding features do not contribute to the model and are effectively excluded.\n",
    "\n",
    "Shrinkage Effect: Lasso tends to shrink the coefficients of less important features towards zero. This helps in simplifying the model and reducing overfitting by focusing on the most significant predictors.\n",
    "\n",
    "In summary, Lasso Regression not only estimates the effect of each feature on the target variable but also performs feature selection by driving some coefficients to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcbd990-02bf-456d-b175-7c62c79fd4d9",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?\n",
    "\n",
    "Ans. In Lasso Regression, the primary tuning parameter is:\n",
    "\n",
    "Alpha (α): This parameter controls the strength of the regularization applied to the model. It affects the model's performance in the following ways:\n",
    "\n",
    "High Alpha: Increases the penalty on the size of the coefficients, which can lead to more coefficients being shrunk to zero. This results in a simpler model with fewer features but might increase bias.\n",
    "\n",
    "Low Alpha: Reduces the penalty, allowing more features to have non-zero coefficients. This can lead to a more complex model with potentially better fit but might increase variance and overfitting.\n",
    "\n",
    "Adjusting the alpha parameter helps balance the trade-off between fitting the training data well and keeping the model simple to generalize better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3e305-4282-449d-b620-e6be86d5c5cf",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "Ans. Yes, Lasso Regression can be used for non-linear regression problems by incorporating non-linear features into the model. Here's how:\n",
    "\n",
    "Feature Transformation: You can create non-linear features by applying transformations (e.g., polynomial features, interaction terms, or using basis functions like splines) to the original predictors.\n",
    "\n",
    "Apply Lasso Regression: Once the non-linear features are generated, you apply Lasso Regression to the transformed dataset. Lasso will then select the most important features, including any non-linear ones, and shrink the others.\n",
    "\n",
    "By expanding the feature space with non-linear transformations, Lasso Regression can capture non-linear relationships while still performing feature selection and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e611e3f4-665d-46d0-b136-3c1ccc45f7af",
   "metadata": {},
   "source": [
    "Q6.What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "Ans. Penalty: Ridge uses L2 (squared coefficients), while Lasso uses L1 (absolute coefficients).\n",
    "\n",
    "Feature Selection: Ridge shrinks coefficients but keeps all features. Lasso can shrink some coefficients to zero, effectively selecting features.\n",
    "\n",
    "Best Use: Ridge is ideal when all features may be important. Lasso is better when only a few features are likely to be significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c39db-a30c-4ab5-83b0-36910168b9a6",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "Ans. Yes, Lasso Regression can handle multicollinearity by selecting one feature among correlated ones and shrinking the others to zero. This feature selection helps in reducing the impact of multicollinearity by keeping only the most important predictor in the model, thus simplifying the model and improving interpretability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a84e0-7253-491f-8200-a0b3d8df1466",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "Ans. \n",
    "The optimal value of the regularization parameter (lambda) in Lasso Regression can be chosen using:\n",
    "\n",
    "Cross-Validation: Split the data into training and validation sets, then try different lambda values. The lambda that minimizes the validation error is selected as the optimal one.\n",
    "\n",
    "Grid Search: Define a range of lambda values and systematically evaluate each using cross-validation. The best lambda is the one that provides the lowest cross-validation error.\n",
    "\n",
    "Regularization Path: Some libraries provide methods to compute the coefficients for a range of lambda values. You can plot these paths and choose the lambda that best balances bias and variance.\n",
    "\n",
    "Cross-validation is the most common and reliable method for selecting the optimal lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782e4fa2-2d70-46cf-93ea-5db78865d821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
